import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# 1. Load Data
df = pd.read_csv('/USA Housing Dataset.csv')

# 2. CLEANING: Remove rows with 0 price
df = df[df['price'] > 0].copy()

# 3. FEATURE ENGINEERING: "One-Hot Encode" the Zip Codes
zip_dummies = pd.get_dummies(df['statezip'], prefix='zip', drop_first=True)

# Merge these new number columns back into our main dataset
df = pd.concat([df, zip_dummies], axis=1)

# 4. SELECT FEATURES
columns_to_drop = ['date', 'street', 'city', 'country', 'statezip', 'price']
X = df.drop(columns_to_drop, axis=1)
y = df['price']

# 5. Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 6. Train Model
model = LinearRegression()
model.fit(X_train, y_train)

# 7. Evaluate
y_pred = model.predict(X_test)
accuracy = r2_score(y_test, y_pred)

print(f"Model Accuracy (R^2): {accuracy:.2f}")
print(f"Number of Features used: {X.shape[1]}")
import matplotlib.pyplot as plt
import seaborn as sns

# Create a scatter plot of actual vs. predicted values
plt.figure(figsize=(10, 7))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--') # Perfect prediction line
plt.title('Actual vs. Predicted House Prices')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.grid(True)
plt.show()
